{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps \n",
    "\n",
    "- Find a dataset of 10,00 most popular packages (use python or js as there the languages I know) \n",
    "- Train a BFE tokenizer on source code dataset\n",
    "- Train an LLM on the tokenized dataset (they use GPT2-774M which has 36 layers, 20 attention heads and a max sequence length of 1024, learning rate of 0.0001, weight decay of 0.01 and cosine learning rate decay schedule) - They trained it on four RTX800 GPUS for one month. I wont be able to do that, so I will used a smaller dataset and a smaller model which requires less compute.\n",
    " \n",
    "\n",
    "The authors in the paper take the original program, apply a security test (codeQL) where the program fails due a bug, then they apply a prompt for the llm to fix the bug from the report codeQL gives. The llm outputs a patch which is then merged into the original program. The program is then tested again to see if the bug is fixed.\n",
    "\n",
    "Ouput - Fixed programm, patch and the test results.\n",
    "\n",
    "Challenges the authors seemed to face:\n",
    "- Prompt engineering. Lots of models can only take in a certain amount of tokens for the context they can ingest. \n",
    "- If the program is too big, the model wont be able to process it, meaning the program needs to be split into smaller chunks.\n",
    "- Prompts should follow LLM documentation - https://www.promptingguide.ai/applications/coding\n",
    "\n",
    "\n",
    "Evaluation:\n",
    "- The authors generate buggy code examples and then tested codex(openAi) LLM which is used by Github Copilot. They then compared the results of the LLM to the results of the authors GPT2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
